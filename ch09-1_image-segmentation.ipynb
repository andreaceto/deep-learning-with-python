{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b664722",
   "metadata": {},
   "source": [
    "# An image segmentation example\n",
    "In this example, we’ll focus on **semantic segmentation**: we’ll be looking once again at images of cats and dogs, and this time we’ll learn how to tell apart the main subject and its background.\n",
    "\n",
    "We’ll work with the [Oxford-IIIT Pets dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/), which contains $7,390$ pictures of various breeds of cats and dogs, together with foreground-background _segmentation masks_ for each picture.\n",
    "\n",
    "A **segmentation mask** is the image-segmentation equivalent of a label: it’s an image the same size as the input image, with a single color channel where each integer value corresponds to the class of the corresponding pixel in the input image. In our case, the pixels of our segmentation masks can take one of three integer values:\n",
    "- 1 (foreground)\n",
    "- 2 (background)\n",
    "- 3 (contour)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5b79a8",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "### Downloadind the dataset\n",
    "You can use the link above to download both images and annotations directories, otherwise you can use the following snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2869d9f",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
    "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
    "!tar -xf images.tar.gz\n",
    "!tar -xf annotations.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd5dea2",
   "metadata": {},
   "source": [
    "The input pictures are stored as **JPG** files in the ```images/``` folder, and the corresponding segmentation mask is stored as a **PNG** file with the same name in the ```annotations/trimaps/``` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0af1cff",
   "metadata": {},
   "source": [
    "### Preparing the input paths\n",
    "Let’s prepare the list of input file paths, as well as the list of the corresponding\n",
    "mask file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09c68f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOCAL VERSION\n",
    "import os\n",
    "from constants import PET_PATH\n",
    "\n",
    "input_dir = os.path.join(PET_PATH, \"images\")\n",
    "target_dir = os.path.join(PET_PATH, \"annotations\", \"trimaps\")\n",
    "\n",
    "input_img_paths = sorted(\n",
    "    [os.path.join(input_dir, fname)\n",
    "     for fname in os.listdir(input_dir)\n",
    "     if fname.endswith(\".jpg\")])\n",
    "target_paths = sorted(\n",
    "    [os.path.join(target_dir, fname)\n",
    "     for fname in os.listdir(target_dir)\n",
    "     if fname.endswith(\".png\") and not fname.startswith(\".\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcb5e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## COLAB VERSION\n",
    "import os\n",
    "\n",
    "input_dir = \"images/\"\n",
    "target_dir = \"annotations/trimaps/\"\n",
    "\n",
    "input_img_paths = sorted(\n",
    "    [os.path.join(input_dir, fname)\n",
    "     for fname in os.listdir(input_dir)\n",
    "     if fname.endswith(\".jpg\")])\n",
    "target_paths = sorted(\n",
    "    [os.path.join(target_dir, fname)\n",
    "     for fname in os.listdir(target_dir)\n",
    "     if fname.endswith(\".png\") and not fname.startswith(\".\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f925b2",
   "metadata": {},
   "source": [
    "### Understanding the dataset\n",
    "What does one of these inputs and its mask look like? Let’s take a quick look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e6dffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(load_img(input_img_paths[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002a9743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_target(target_array):\n",
    "    # The original labels are 1, 2 and 3. We subtract 1 so that the labels range from 0 to 2.\n",
    "    normalized_array = (target_array.astype(\"uint8\") - 1) * 127  # we multiply by 127 so that the labels become 0 (black), 127 (gray), 254 (near-white).\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(normalized_array[:, :, 0])\n",
    "\n",
    "img = img_to_array(load_img(target_paths[9], color_mode=\"grayscale\"))\n",
    "display_target(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072c706d",
   "metadata": {},
   "source": [
    "### Training-Validation split\n",
    "Next, let’s load our inputs and targets into two NumPy arrays, and let’s split the arrays into a training and a validation set. Since the dataset is very small, we can just load everything into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b3524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "img_size = (200, 200)  # We resize everything to 200x200\n",
    "num_imgs = len(input_img_paths)\n",
    "\n",
    "# We Shuffle the file paths (they were originally sorted by breed).\n",
    "# We use the same seed (1337) in both statements to ensure that the input paths and target paths stay in the same order.\n",
    "random.Random(1337).shuffle(input_img_paths)\n",
    "random.Random(1337).shuffle(target_paths)\n",
    "\n",
    "def path_to_input_image(path):\n",
    "    return img_to_array(load_img(path, target_size=img_size))\n",
    "\n",
    "def path_to_target(path):\n",
    "    img = img_to_array(\n",
    "        load_img(path, target_size=img_size, color_mode=\"grayscale\"))\n",
    "    img = img.astype(\"uint8\") - 1\n",
    "    return img\n",
    "\n",
    "input_imgs = np.zeros((num_imgs,) + img_size + (3,), dtype=\"float32\")\n",
    "targets = np.zeros((num_imgs,) + img_size + (1,), dtype=\"uint8\")\n",
    "for i in range(num_imgs):\n",
    "    # Load all images in the input_imgs float32 array. The inputs have three channels (RBG values)\n",
    "    input_imgs[i] = path_to_input_image(input_img_paths[i])\n",
    "    # Load their masks in the targets uint8 array (same order). The targets have a single channel (which contains integer labels).\n",
    "    targets[i] = path_to_target(target_paths[i])\n",
    "\n",
    "num_val_samples = 1000  # We'll use 1'000 samples for validation\n",
    "train_input_imgs = input_imgs[:-num_val_samples]\n",
    "train_targets = targets[:-num_val_samples]\n",
    "val_input_imgs = input_imgs[-num_val_samples:]\n",
    "val_targets = targets[-num_val_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ec5673",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d359d5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model(img_size, num_classes):\n",
    "    inputs = keras.Input(shape=img_size + (3,))\n",
    "    x = layers.Rescaling(1./255)(inputs)\n",
    "\n",
    "    x = layers.Conv2D(64, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.Conv2D(128, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.Conv2D(256, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(256, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.Conv2DTranspose(256, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "    x = layers.Conv2DTranspose(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.Conv2DTranspose(128, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "\n",
    "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "model = get_model(img_size=img_size, num_classes=3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48cd843",
   "metadata": {},
   "source": [
    "The first half of the model closely resembles the kind of convnet you’d use for image classification: a stack of ```Conv2D``` layers, with gradually increasing filter sizes. We downsample our images three times by a factor of two each, ending up with activations of size $(25, 25, 256)$. The purpose of this first half is to encode the images into smaller feature maps, where each spatial location (or pixel) contains information about a large spatial chunk of the original image.\n",
    "\n",
    "One important difference between the first half of this model and the classification models you’ve seen before is the way we do downsampling: in the classification convnets from the last notebook, we used ```MaxPooling2D``` layers to downsample feature maps. Here, we downsample by adding strides to every other convolution layer. We do this because, in the case of image segmentation, we care a lot about the spatial location of information in the image, since we need to produce per-pixel target masks as output of the model. When you do $2 × 2$ max pooling, you are completely destroying location information within each pooling window: you return one scalar value per window, with zero knowledge of which of the four locations in the windows the value came from.\n",
    "\n",
    "The second half of the model is a stack of ```Conv2DTranspose``` layers. _What are those?_\n",
    "\n",
    "The output of the first half of the model is a feature map of shape $(25, 25, 256)$, but we want our final output to have the same shape as the target masks, $(200, 200,3)$. Therefore, we need to apply a kind of inverse of the transformations we’ve applied so far—something that will **upsample** the feature maps instead of downsampling them. That’s the purpose of the Conv2DTranspose layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63263f9",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97460cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"oxford_segmentation.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "\n",
    "history = model.fit(train_input_imgs, train_targets,\n",
    "                    epochs=50,\n",
    "                    callbacks=callbacks,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(val_input_imgs, val_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfc53d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(history.history[\"loss\"]) + 1)\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e0eee8",
   "metadata": {},
   "source": [
    "We start overfitting midway, around epoch 25."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35a27f6",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "Let’s reload our best performing model according to the validation loss, and demonstrate how to use it to predict a segmentation mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf31574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import array_to_img\n",
    "\n",
    "model = keras.models.load_model(\"oxford_segmentation.keras\")\n",
    "\n",
    "i = 4\n",
    "test_image = val_input_imgs[i]\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(array_to_img(test_image))\n",
    "\n",
    "mask = model.predict(np.expand_dims(test_image, 0))[0]\n",
    "\n",
    "def display_mask(pred):\n",
    "    mask = np.argmax(pred, axis=-1)\n",
    "    mask *= 127\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(mask)\n",
    "\n",
    "display_mask(mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
