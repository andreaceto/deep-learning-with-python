{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec343ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f31f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "import os, pathlib\n",
    "from constants import DATASETS_DIR_PATH, CATS_VS_DOGS_PATH\n",
    "\n",
    "# Path to the directory where the original dataset was uncompressed\n",
    "original_dir = pathlib.Path(CATS_VS_DOGS_PATH)\n",
    "\n",
    "# Directory where we will store our smaller dataset\n",
    "new_base_dir = pathlib.Path(os.path.join(DATASETS_DIR_PATH, \"dogs_vs_cats_small\"))\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    new_base_dir / \"train\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    new_base_dir / \"validation\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    new_base_dir / \"test\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add76d31",
   "metadata": {},
   "source": [
    "# Leveraging a pretrained model\n",
    "A common and highly effective approach to deep learning on small image datasets is to use a _pretrained model_.\n",
    "\n",
    "A **pretrained model** is a model that was previously trained on a large dataset, typically on a large-scale image-classification task. If this original dataset is large enough and general enough, the spatial hierarchy of features learned by the pretrained model can effectively act as a generic model of the visual world, and hence, its features can prove useful for many different computer vision problems, even though these new problems may involve completely different classes than those of the original task.\n",
    "\n",
    "Let’s consider a large convnet trained on the **ImageNet** dataset ($1.4 million$ labeled images and $1,000$ different classes). ImageNet contains many animal classes, including different species of cats and dogs, and you can thus expect it to perform well on the dogs-versus-cats classification problem.\n",
    "\n",
    "We’ll use the **VGG16** architecture. Although it’s an older model, far from the current state of the art and somewhat heavier than many other recent models, its architecture is similar to what we saw in the previous notebooks.\n",
    "\n",
    "There are two ways to use a pretrained model: feature extraction and fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1533ebb4",
   "metadata": {},
   "source": [
    "## Feature extraction with a pretrained model\n",
    "Because the ImageNet class set contains multiple dog and cat classes, it’s likely to be beneficial to reuse the information contained in the densely connected layers of the original model. But we’ll choose not to, in order to cover the more general case where the class set of the new problem doesn’t overlap the class set of the original model.\n",
    "\n",
    "Let’s put this into practice by using the convolutional base of the VGG16 network, trained on ImageNet, to extract interesting features from cat and dog images, and then train a dogs-versus-cats classifier on top of these features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95d111b",
   "metadata": {},
   "source": [
    "### Instantiating the VGG16 convolutional base\n",
    "The VGG16 model, among others, comes prepackaged with Keras. You can import it from the ```keras.applications``` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9a7f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "conv_base = keras.applications.vgg16.VGG16(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False,\n",
    "    input_shape=(180, 180, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfe2f47",
   "metadata": {},
   "source": [
    "We pass three arguments to the constructor:\n",
    "- ```weights``` specifies the weight checkpoint from which to initialize the model.\n",
    "- ```include_top``` refers to including (or not) the densely connected classifier on top of the network. By default, this densely connected classifier corresponds to the $1,000$ classes from ImageNet. Because we intend to use our own densely connected classifier (with only two classes: cat and dog), we don’t need to include it.\n",
    "- ```input_shape``` is the shape of the image tensors that we’ll feed to the network. This argument is purely optional: if we don’t pass it, the network will be able to process inputs of any size. Here we pass it so that we can visualize (in the following summary) how the size of the feature maps shrinks with each new convolution and pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fff4f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae9ba9c",
   "metadata": {},
   "source": [
    "The final feature map has shape ```(5, 5, 512)```. That’s the feature map on top of which we’ll stick a densely connected classifier.\n",
    "\n",
    "At this point, there are two ways we could proceed:\n",
    "1. **Run the convolutional base over our dataset, record its output to a NumPy array on disk, and then use this data as input to a standalone, densely connected classifier**. This solution is fast and cheap to run, because it only requires running the convolutional base once for every input image, and the convolutional base is by far the most expensive part of the pipeline. But for the same reason, this technique won’t allow us to use data augmentation.\n",
    "2. **Extend the model we have (```conv_base```) by adding ```Dense``` layers on top, and run the whole thing from end to end on the input data**. This will allow us to use data augmentation, because every input image goes through the convolutional base every time it’s seen by the model. But for the same reason, this technique is far more expensive than the first.\n",
    "\n",
    "Let's see both techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941628cc",
   "metadata": {},
   "source": [
    "## Fast Feature Extraction without data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f88e0fb",
   "metadata": {},
   "source": [
    "### Extracting the VGG16 features and corresponding labels\n",
    "We’ll start by extracting features as NumPy arrays by calling the ```predict()``` method of the ```conv_base``` model on our training, validation, and testing datasets.\n",
    "\n",
    "Importantly, ```predict()``` only expects images, not labels, but our current dataset yields batches that contain both images and their labels. Moreover, the VGG16 model expects inputs that are preprocessed with the function ```keras.applications.vgg16.preprocess_input()```, which scales pixel values to an appropriate range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea98a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_features_and_labels(dataset):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    for images, labels in dataset:\n",
    "        preprocessed_images = keras.applications.vgg16.preprocess_input(images)\n",
    "        features = conv_base.predict(preprocessed_images)\n",
    "        all_features.append(features)\n",
    "        all_labels.append(labels)\n",
    "    return np.concatenate(all_features), np.concatenate(all_labels)\n",
    "\n",
    "train_features, train_labels =  get_features_and_labels(train_dataset)\n",
    "val_features, val_labels =  get_features_and_labels(validation_dataset)\n",
    "test_features, test_labels =  get_features_and_labels(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adef4fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fad1bb",
   "metadata": {},
   "source": [
    "### Defining and training the densely connected classifier\n",
    "At this point, we can define our densely connected classifier (note the use of dropout for regularization) and train it on the data and labels that we just recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c954d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = keras.Input(shape=(5, 5, 512))\n",
    "x = layers.Flatten()(inputs)\n",
    "x = layers.Dense(256)(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "      filepath=\"feature_extraction.keras\",\n",
    "      save_best_only=True,\n",
    "      monitor=\"val_loss\")\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_features, train_labels,\n",
    "    epochs=20,\n",
    "    validation_data=(val_features, val_labels),\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daa5a00",
   "metadata": {},
   "source": [
    "### Plotting the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5bfc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1609a33",
   "metadata": {},
   "source": [
    "We reach a validation accuracy of about 97%—much better than we achieved in the previous notebook with the small model trained from scratch.\n",
    "\n",
    "However, the plots also indicate that we’re overfitting almost from the start despite using dropout with a fairly large rate. That’s because this technique doesn’t use data augmentation, which is essential for preventing overfitting with small image datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b174e139",
   "metadata": {},
   "source": [
    "## Feature extraction together with data augmentation\n",
    "Now let’s review the second technique which allows us to use data augmentation during training: creating a model that chains the ```conv_base``` with a new dense classifier, and training it end to end on the inputs.\n",
    "\n",
    "In order to do this, we will first **freeze the convolutional base**. Freezing a layer or set of layers means preventing their weights from being updated during training. If we don’t do this, the representations that were previously learned by the convolutional base will be modified during training. Because the Dense layers on top are randomly initialized, very large weight updates would be propagated through the network, effectively destroying the representations previously learned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d15b8a",
   "metadata": {},
   "source": [
    "### Instantiating and freezing the VGG16 convolutional base\n",
    "In Keras, we freeze a layer or model by setting its ```trainable``` attribute to ```False```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c773f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base  = keras.applications.vgg16.VGG16(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False)\n",
    "\n",
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e89512",
   "metadata": {},
   "source": [
    "### Printing the list of trainable weights before and after freezing\n",
    "Setting ```trainable``` to ```False``` empties the list of trainable weights of the layer or model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1a3c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = True\n",
    "print(\"This is the number of trainable weights \"\n",
    "      \"before freezing the conv base:\", len(conv_base.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd46e300",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = False\n",
    "print(\"This is the number of trainable weights \"\n",
    "      \"after freezing the conv base:\", len(conv_base.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cb5c65",
   "metadata": {},
   "source": [
    "### Adding a data augmentation stage and a classifier to the convolutional base\n",
    "Now we can create a new model that chains together:\n",
    "1. A data augmentation stage;\n",
    "2. Our frozen convolutional base;\n",
    "3. A dense classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554e141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "x = data_augmentation(inputs)  # Apply the data augmentation\n",
    "x = keras.applications.vgg16.preprocess_input(x)  # Apply input value scaling\n",
    "x = conv_base(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256)(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b761603",
   "metadata": {},
   "source": [
    "With this setup, only the weights from the two ```Dense``` layers that we added will be trained. That’s a total of four weight tensors: two per layer (the main weight matrix and the bias vector).\n",
    "\n",
    "Note that in order for these changes to take effect, you must first compile the model. If you ever modify weight trainability after compilation, you should then recompile the model, or these changes will be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d453014",
   "metadata": {},
   "source": [
    "### Training\n",
    "Let’s train our model. Thanks to data augmentation, it will take much longer for the model to start overfitting, so we can train for more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c586708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"feature_extraction_with_data_augmentation.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=50,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599c6d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1081f962",
   "metadata": {},
   "source": [
    "### Evaluating the model on the test set\n",
    "Let’s check the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f46037",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = keras.models.load_model(\n",
    "    \"feature_extraction_with_data_augmentation.keras\")\n",
    "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77defac7",
   "metadata": {},
   "source": [
    "## Fine-tuning a pretrained model\n",
    "Another widely used technique for model reuse, complementary to feature extraction, is **fine-tuning**.\n",
    "\n",
    "The steps for fine-tuning a network are as follows:\n",
    "1. Add our custom network on top of an already-trained base network.\n",
    "2. Freeze the base network.\n",
    "3. Train the part we added.\n",
    "4. Unfreeze some layers in the base network. (Note that you should not unfreeze “batch normalization” layers, which are not relevant here since there are no such layers in VGG16.)\n",
    "5. Jointly train both these layers and the part we added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3317a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ca6f63",
   "metadata": {},
   "source": [
    "We’ll fine-tune the last three convolutional layers, which means all layers up to ```block4_pool``` should be frozen, and the layers ```block5_conv1```, ```block5_conv2```, and ```block5_conv3``` should be trainable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121657ea",
   "metadata": {},
   "source": [
    "### Freezing all layers until the fourth from the last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6405e27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = True\n",
    "for layer in conv_base.layers[:-4]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d74781",
   "metadata": {},
   "source": [
    "### Fine-tuning the model\n",
    "Now we can begin fine-tuning the model. We’ll do this with the ```RMSprop``` optimizer, using a very low learning rate. The reason for using a low learning rate is that we want to limit the magnitude of the modifications we make to the representations of the three layers we’re fine-tuning. Updates that are too large may harm these representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f0188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=keras.optimizers.RMSprop(learning_rate=1e-5),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"fine_tuning.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=30,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da364cf",
   "metadata": {},
   "source": [
    "We can finally evaluate this model on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432ab6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"fine_tuning.keras\")\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
