{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d43af399",
   "metadata": {},
   "source": [
    "# Two approaches for representing groups of words: Sets and sequences\n",
    "We’ll demonstrate each approach on a well-known text classification benchmark: the IMDB movie review sentiment-classification dataset.\n",
    "\n",
    "In the previous notebooks (ch. 4 and 5) we worked with a prevectorized version of the IMDB dataset; now, let’s process the raw IMDB text data, just like you would do when approaching a new text-classification problem in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50686a1d",
   "metadata": {},
   "source": [
    "## Preparing the IMDB movie reviews data\n",
    "Let’s start by downloading the dataset from the Stanford page of Andrew Maas and uncompressing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9528bdc4",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f774b628",
   "metadata": {},
   "source": [
    "In total, there are $25,000$ text files for training and another $25,000$ for testing.\n",
    "\n",
    "There’s also a train/unsup subdirectory in there, which we don’t need. Let’s delete it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7c11fe",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d7f9e4",
   "metadata": {},
   "source": [
    "Whether you’re working with text data or image data, remember to always inspect what your data looks like before you dive into modeling it. It will ground your intuition about what your model is actually doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ffd0ee",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!cat aclImdb/train/pos/4077_10.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c46518",
   "metadata": {},
   "source": [
    "Next, let’s prepare a validation set by setting apart $20%$ of the training text files in a new directory, ```aclImdb/val```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd975e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    # Shuffle the list of training files using a seed, to ensure we get the same validation set every time\n",
    "    random.Random(1337).shuffle(files)\n",
    "    # Take 20% of the training files to use for validation\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        # Move the files to 'aclImdb/val/neg' and 'aclImdb/val/pos'\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fe5461",
   "metadata": {},
   "source": [
    "In the same way we used ```image_dataset_from_directory``` to create a batched _Dataset_ in ch. 8, we can use ```text_dataset_from_directory``` for text files.\n",
    "\n",
    "Let’s create three _Dataset_ objects for training, validation, and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bd2452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e42f83c",
   "metadata": {},
   "source": [
    "### Displaying the shapes and dtypes of the first batch\n",
    "These datasets yield inputs that are TensorFlow _tf.string_ tensors and targets that are\n",
    "_int32_ tensors encoding the value “0” or “1.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d46b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e84c107",
   "metadata": {},
   "source": [
    "## Processing words as a set: The bag-of-words approach\n",
    "The simplest way to encode a piece of text for processing by a machine learning model is to discard order and treat it as a set (a “bag”) of tokens.\n",
    "\n",
    "You could either look at individual words (unigrams), or try to recover some local order information by looking at groups of consecutive token (N-grams)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662da86e",
   "metadata": {},
   "source": [
    "### SINGLE WORDS (UNIGRAMS) WITH BINARY ENCODING\n",
    "If you use a bag of single words, the sentence _“the cat sat on the mat”_ becomes:\n",
    "\n",
    "{\"cat\", \"mat\", \"on\", \"sat\", \"the\"}\n",
    "\n",
    "The main advantage of this encoding is that you can represent an entire text as a single vector, where each entry is a presence indicator for a given word. For instance, using binary encoding (multi-hot), you’d encode a text as a vector with as many dimensions as there are words in your vocabulary—with 0s almost everywhere and some 1s for dimensions that encode words present in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2298c7c",
   "metadata": {},
   "source": [
    "### Preprocessing our datasets with a _TextVectorization_ layer\n",
    "First, let’s process our raw text datasets with a _TextVectorization_ layer so that they yield multi-hot encoded binary word vectors. Our layer will only look at single words (that is to say, _unigrams_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60393cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    # Limit the vocabulary to the 20,000 most frequent words\n",
    "    max_tokens=20000,\n",
    "    # Encode the output tokens as multi-hot binary vectors\n",
    "    output_mode=\"multi_hot\",\n",
    ")\n",
    "\n",
    "# Prepare a dataset that only yields raw text inputs (no labels)\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "# Use that dataset to index the dataset vocabulary via the adapt() method\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "# Prepare processed versions of our training, validation, and test dataset\n",
    "binary_1gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4) # Make sure to specify num_parallel_calls to leverage multiple CPU cores\n",
    "binary_1gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_1gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a425db3",
   "metadata": {},
   "source": [
    "### Inspecting the output of our binary unigram dataset\n",
    "You can try to inspect the output of one of these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efe9ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1d4b51",
   "metadata": {},
   "source": [
    "### Our model-building utility\n",
    "Next, let’s write a reusable model-building function that we’ll use in all of our experiments in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a829e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2302940",
   "metadata": {},
   "source": [
    "### Training and testing the binary unigram model\n",
    "Finally, let’s train and test our model.\n",
    "\n",
    "We call ```cache()``` on the datasets to cache them in memory: this way, we will only do the preprocessing\n",
    "once, during the first epoch, and we’ll reuse the preprocessed texts for the following epochs. This can\n",
    "only be done if the data is small enough to fit in memory.\n",
    "\n",
    "Note that in this case, since the dataset is a balanced two-class classification dataset (there are as many positive samples as negative samples), the “naive baseline” we could reach without training an actual model would only be $50%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdc5d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(binary_1gram_train_ds.cache(),\n",
    "          validation_data=binary_1gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_1gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523f2da1",
   "metadata": {},
   "source": [
    "### BIGRAMS WITH BINARY ENCODING\n",
    "Of course, discarding word order is very reductive, because even atomic concepts can be expressed via multiple words: the term “United States” conveys a concept that is quite distinct from the meaning of the words “states” and “united” taken separately.\n",
    "\n",
    "For this reason, you will usually end up re-injecting local order information into your bag-of-words representation by looking at N-grams rather than single words (most commonly, bigrams).\n",
    "\n",
    "With bigrams, our sentence becomes:\n",
    "\n",
    "{\"the\", \"the cat\", \"cat\", \"cat sat\", \"sat\",\n",
    "\"sat on\", \"on\", \"on the\", \"the mat\", \"mat\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d29bf75",
   "metadata": {},
   "source": [
    "### Configuring the _TextVectorization_ layer to return bigrams\n",
    "The _TextVectorization_ layer can be configured to return arbitrary N-grams: bigrams, trigrams, etc. Just pass an ```ngrams=N``` argument as in the following listing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f90fe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6e147d",
   "metadata": {},
   "source": [
    "### Training and testing the binary bigram model\n",
    "Let’s test how our model performs when trained on such binary-encoded bags of bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06974be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "binary_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(binary_2gram_train_ds.cache(),\n",
    "          validation_data=binary_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a0b2c5",
   "metadata": {},
   "source": [
    "### BIGRAMS WITH TF-IDF ENCODING\n",
    "You can also add a bit more information to this representation by counting how many times each word or N-gram occurs, that is to say, by taking the histogram of the words over the text:\n",
    "\n",
    "{\"the\": 2, \"the cat\": 1, \"cat\": 1, \"cat sat\": 1, \"sat\": 1,\n",
    "\"sat on\": 1, \"on\": 1, \"on the\": 1, \"the mat: 1\", \"mat\": 1}\n",
    "\n",
    "If you’re doing text classification, knowing how many times a word occurs in a sample is critical: any sufficiently long movie review may contain the word “terrible” regardless of sentiment, but a review that contains many instances of the word “terrible” is likely a negative one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a45ee11",
   "metadata": {},
   "source": [
    "### Configuring the _TextVectorization_ layer to return token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272f4405",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"count\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc9bdb3",
   "metadata": {},
   "source": [
    "### Configuring _TextVectorization_ to return TF-IDF-weighted outputs\n",
    "Now, of course, some words are bound to occur more often than others no matter what the text is about.\n",
    "\n",
    "The words _“the”_, _“a”_, _“is”_, and _“are”_ will always dominate your word count histograms, drowning out other words $—$ despite being pretty much useless features in a classification context.\n",
    "\n",
    "We can address this via **normalization**. The best practice is to go with something called **TF-IDF normalization**.\n",
    "\n",
    "TF-IDF stands for _“term frequency, inverse document frequency”_. It weights a given term by taking “term frequency,” how many times the term appears in the current document, and dividing it by a measure of “document frequency,” which estimates how often the term comes up across the dataset. In other words, if a given term appears frequently across all documents in a dataset, then it's not informative about a specific document.\n",
    "\n",
    "TF-IDF is so common that it’s built into the _TextVectorization_ layer. All you need to do to start using it is to switch the output_mode argument to _\"tf_idf\"_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ae18ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"tf_idf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca852f68",
   "metadata": {},
   "source": [
    "### Training and testing the TF-IDF bigram model\n",
    "Let’s train a new model with this scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6bbb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "tfidf_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(tfidf_2gram_train_ds.cache(),\n",
    "          validation_data=tfidf_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7460617f",
   "metadata": {},
   "source": [
    "These past few examples clearly show that word order matters: manual engineering of order-based features, such as bigrams, yields a nice accuracy boost. Now remember: the history of deep learning is that of a move away from manual feature engineering, toward letting models learn their own features from exposure to data alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5371207f",
   "metadata": {},
   "source": [
    "## Processing words as a sequence: The sequence model approach\n",
    "To implement a sequence model, you’d start by representing your input samples as sequences of integer indices (one integer standing for one word).\n",
    "\n",
    "Then, you’d map each integer to a vector to obtain vector sequences.\n",
    "\n",
    "Finally, you’d feed these sequences of vectors into a stack of layers that could cross-correlate features from adjacent vectors, such as a 1D convnet, a RNN, or a Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465689d7",
   "metadata": {},
   "source": [
    "### Preparing integer sequence datasets\n",
    "First, let’s prepare datasets that return integer sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faf207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,  # to keep a manageable input size, we’ll truncate the inputs after the first 600 words (only 5% of reviews are longer)\n",
    ")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b2f4da",
   "metadata": {},
   "source": [
    "### A sequence model built on one-hot encoded vector sequences\n",
    "Next, let’s make a model. The simplest way to convert our integer sequences to vector sequences is to one-hot encode the integers (each dimension would represent one possible term in the vocabulary). On top of these one-hot vectors, we’ll add a simple bidirectional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982837ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = tf.one_hot(inputs, depth=max_tokens)  # Encode the integers into binary 20'000 dimensional vectors\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)  # Add a bidirectional LSTM\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)  # Finally, add a classification layer\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b8b066",
   "metadata": {},
   "source": [
    "### Training a first basic sequence model\n",
    "Now, let’s train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe538bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b46c2b",
   "metadata": {},
   "source": [
    "This model trains very slowly, especially compared to the lightweight model of the previous section. This is because our inputs are quite large: each input sample is encoded as a matrix of size $(600, 20000)$ (600 words per sample, 20,000 possible words). That’s $12,000,000$ floats for a single movie review. Our bidirectional LSTM has a lot of work to do.\n",
    "\n",
    "Second, the model doesn’t perform nearly as well as our (very fast) binary unigram model. Clearly, using one-hot encoding to turn words into vectors, which was the simplest thing we could do, wasn’t a great idea. There’s a better way: **_word embeddings_**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
