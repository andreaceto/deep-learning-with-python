{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a322a0",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz\n",
    "!rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3f4db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "from tensorflow import keras\n",
    "\n",
    "batch_size = 32\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edc721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125f6e42",
   "metadata": {},
   "source": [
    "# Understanding word embeddings\n",
    "Word embeddings are low-dimensional floating-point vector representations of words that map human language into a structured geometric space.\n",
    "\n",
    "Besides being dense representations, word embeddings are also structured representations, and their structure is learned from data. Similar words get embedded in close locations, and further, specific directions in the embedding space are meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3328c7",
   "metadata": {},
   "source": [
    "## Learning word embeddings with the `Embedding` layer\n",
    "_Is there some ideal word-embedding space that would perfectly map human language and could be used for any natural language processing task?_ \n",
    "\n",
    "Possibly, but we have yet to compute anything of the sort. Also, there is no such a thing as human language $—$ there are many different languages, and they aren’t isomorphic to one another, because a language is the reflection of a specific culture and a specific context. But more pragmatically, what makes a good word-embedding space depends heavily on your task.\n",
    "\n",
    "It’s thus reasonable to **learn** a new embedding space with every new task. Fortunately, backpropagation makes this easy, and Keras makes it even easier. It’s about learning the weights of a layer: the _Embedding_ layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dfb311",
   "metadata": {},
   "source": [
    "### Instantiating an `Embedding` layer\n",
    "The Embedding layer is best understood as a dictionary that maps integer indices (which stand for specific words) to dense vectors.\n",
    "\n",
    "It takes as input a rank-2 tensor of integers, of shape `(batch_size, sequence_length)`, where each entry is a sequence of integers. The layer then returns a 3D floating-point tensor of shape `(batch_size,sequence_length, embedding_dimensionality)`. It’s effectively a dictionary lookup.\n",
    "\n",
    "When you instantiate an Embedding layer, its weights (its internal dictionary of token vectors) are initially random, just as with any other layer. During training, these word vectors are gradually adjusted via backpropagation, structuring the space into something the downstream model can exploit. Once fully trained, the embedding space will show a lot of structure—a kind of structure specialized for the specific problem for which you’re training your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde3bdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc947e59",
   "metadata": {},
   "source": [
    "### Model that uses an `Embedding` layer trained from scratch\n",
    "Let’s build a model that includes an Embedding layer and benchmark it on our task (the same of [previous notebook](ch11-1_sets-vs-sequences.ipynb))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1628e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"embeddings_bidir_gru.keras\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0f4b54",
   "metadata": {},
   "source": [
    "It trains much faster than the one-hot model (since the LSTM only has to process $256$-dimensional vectors instead of $20,000$-dimensional), and its test accuracy is comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6509d12d",
   "metadata": {},
   "source": [
    "#### Understanding Padding and masking\n",
    "One thing that’s slightly hurting model performance here is that our input sequences are full of zeros. This comes from our use of the `output_sequence_length=max_length` option in _TextVectorization_ (with max_length equal to 600): sentences longer than $600$ tokens are truncated to a length of $600$ tokens, and sentences shorter than $600$ tokens are padded with zeros at the end so that they can be concatenated\n",
    "together with other sequences to form contiguous batches.\n",
    "\n",
    "We’re using a bidirectional RNN: two RNN layers running in parallel, with one processing the tokens in their natural order, and the other processing the same tokens in reverse. The RNN that looks at the tokens in their natural order will spend its last iterations seeing only vectors that encode padding $-$ possibly for several hundreds of iterations if the original sentence was short. The information stored in the internal state of the RNN will gradually fade out as it gets exposed to these meaningless inputs.\n",
    "\n",
    "We need some way to tell the RNN that it should skip these iterations. There’s an API for that: _masking_.\n",
    "\n",
    "The Embedding layer is capable of generating a _“mask”_ that corresponds to its input data. This mask is a tensor of ones and zeros (or True/False booleans), of shape `(batch_size, sequence_length)`, where the entry $mask[i, t]$ indicates where timestep $t$ of sample $i$ should be skipped or not (the timestep will be skipped if $mask[i, t]$ is $0$ or $False$, and processed otherwise).\n",
    "\n",
    "By default, this option isn’t active $-$ you can turn it on by passing `mask_zero=True` to your Embedding layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a2cfff",
   "metadata": {},
   "source": [
    "### Using an Embedding layer with masking enabled\n",
    "Let’s try retraining our model with masking enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec6c5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(\n",
    "    input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"embeddings_bidir_gru_with_masking.keras\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9e62e8",
   "metadata": {},
   "source": [
    "## Using pretrained word embeddings\n",
    "Sometimes you have so little training data available that you can’t use your data alone to learn an appropriate task-specific embedding of your vocabulary. In such cases, instead of learning word embeddings jointly with the problem you want to solve, you can load embedding vectors from a precomputed embedding space that you know is highly structured and exhibits useful properties $—$ one that captures generic aspects of language structure.\n",
    "\n",
    "There are various precomputed databases of word embeddings that you can download and use in a Keras Embedding layer. **Word2vec** is one of them. Another popular one is called **Global Vectors for Word Representation** (GloVe).\n",
    "\n",
    "Let’s look at how you can get started using GloVe embeddings in a Keras model. The same method is valid for Word2Vec embeddings or any other word-embedding database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96919658",
   "metadata": {},
   "source": [
    "### Downloading and unzipping the GloVe word-embeddings\n",
    "First, let’s download the GloVe word embeddings precomputed on the **2014 English Wikipedia dataset**.\n",
    "\n",
    "It’s an $822$ MB zip file containing $100$-dimensional embedding vectors for $400,000$ words (or non-word tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a27d6ba",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2128fc90",
   "metadata": {},
   "source": [
    "### Parsing the GloVe word-embeddings file\n",
    "Let’s parse the unzipped file (a .txt file) to build an index that maps words (as strings) to their vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e0d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "path_to_glove_file = \"glove.6B.100d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(f\"Found {len(embeddings_index)} word vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dcc0e2",
   "metadata": {},
   "source": [
    "### Preparing the GloVe word-embeddings matrix\n",
    "Next, let’s build an embedding matrix that you can load into an Embedding layer. It must be a matrix of shape `(max_words, embedding_dim)`, where each entry $i$ contains the _embedding_dim_ $-$ dimensional vector for the word of index $i$ in the reference word index (built during tokenization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879577dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "# Retrieve the vocabulary indexed by our previous TextVectorization layer\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "# Use it to create a mapping from words to their index in the vocabulary\n",
    "word_index = dict(zip(vocabulary, range(len(vocabulary))))  \n",
    "\n",
    "# Prepare a matrix that we’ll fill with the GloVe vectors\n",
    "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
    "\n",
    "# Fill entry i in the matrix with the word vector for index i\n",
    "# Words not found in the embedding index will be all zeros\n",
    "for word, i in word_index.items():\n",
    "\tif i < max_tokens:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e64fc56",
   "metadata": {},
   "source": [
    "### Loading the pretrained embeddings in the Embedding layer\n",
    "Finally, we use a Constant initializer to load the pretrained embeddings in an Embedding layer. So as not to disrupt the pretrained representations during training, we freeze the layer via `trainable=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c12d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(\n",
    "    max_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    "    mask_zero=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec098a1",
   "metadata": {},
   "source": [
    "### Model that uses a pretrained Embedding layer\n",
    "We’re now ready to train a new model $—$ identical to our previous model, but leveraging the $100$-dimensional pretrained GloVe embeddings instead of $128$-dimensional learned embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = embedding_layer(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6993ad07",
   "metadata": {},
   "source": [
    "On this particular task, pretrained embeddings aren’t very helpful, because the dataset contains enough samples that it is possible to learn a specialized enough embedding space from scratch. However, leveraging pretrained embeddings can be very helpful when you’re working with a smaller dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
